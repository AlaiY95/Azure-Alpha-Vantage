{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_secret(key_name, env_var_name):\n",
    "    \"\"\"Get a secret from credentials.json file\"\"\"\n",
    "    try:\n",
    "        # Try to read from credentials file\n",
    "        credentials_path = \"/dbfs/FileStore/configs/credentials.json\"\n",
    "        with open(credentials_path, 'r') as f:\n",
    "            credentials = json.load(f)\n",
    "            print(f\"Loaded credentials from {credentials_path}\")\n",
    "            \n",
    "        if key_name.replace(\"-\", \"_\") in credentials:\n",
    "            return credentials[key_name.replace(\"-\", \"_\")]\n",
    "        else:\n",
    "            raise ValueError(f\"Key {key_name} not found in credentials file\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Fall back to environment variables\n",
    "        load_dotenv()\n",
    "        env_value = os.getenv(env_var_name)\n",
    "        \n",
    "        if not env_value:\n",
    "            raise ValueError(f\"Could not find secret {key_name} in credentials or environment\")\n",
    "        \n",
    "        return env_value\n",
    "    \n",
    "# Load configuration\n",
    "def load_config():\n",
    "    \"\"\"Load configuration from config.json\"\"\"\n",
    "    config_path = \"/dbfs/FileStore/configs/config.json\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # test cell\n",
    "# Test if credentials are accessible\n",
    "try:\n",
    "    api_key = get_secret(\"alpha-vantage-api-key\", \"ALPHA_VANTAGE_API_KEY\")\n",
    "    print(f\"Successfully retrieved API key: {api_key[:4]}...{api_key[-4:]}\")\n",
    "    \n",
    "    # Test if we can call the Alpha Vantage API\n",
    "    import requests\n",
    "    test_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=MSFT&outputsize=compact&apikey={api_key}\"\n",
    "    response = requests.get(test_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"Time Series (Daily)\" in data:\n",
    "            print(\"API test successful! Received time series data.\")\n",
    "            print(f\"Data points: {len(data['Time Series (Daily)'])}\")\n",
    "        else:\n",
    "            print(f\"API returned: {data}\")\n",
    "    else:\n",
    "        print(f\"API request failed with status code: {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error testing credentials: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "print(f\"Config loaded:\\n {config}\")\n",
    "symbols = config[\"stock_symbols\"]\n",
    "bronze_path = config[\"data_storage\"][\"bronze_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define Schema\n",
    "# Define schema for stock data\n",
    "stock_schema = StructType([\n",
    "    StructField(\"symbol\", StringType(), False),\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"open\", DoubleType(), True),\n",
    "    StructField(\"high\", DoubleType(), True),\n",
    "    StructField(\"low\", DoubleType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"volume\", LongType(), True),\n",
    "    StructField(\"ingestion_date\", TimestampType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# API Data Fetching\n",
    "# COMMAND ----------\n",
    "def fetch_alpha_vantage_data(symbol, api_key):\n",
    "    \"\"\"Fetch stock data from Alpha Vantage API\"\"\"\n",
    "    url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check for valid response\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data for {symbol}: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Check for API errors or limits\n",
    "    if \"Error Message\" in data:\n",
    "        print(f\"API Error for {symbol}: {data['Error Message']}\")\n",
    "        return []\n",
    "    \n",
    "    if \"Note\" in data:  # API limit reached\n",
    "        print(f\"API Limit reached: {data['Note']}\")\n",
    "        return []\n",
    "        \n",
    "    # Check if we have data\n",
    "    if \"Time Series (Daily)\" not in data:\n",
    "        print(f\"No data found for {symbol}\")\n",
    "        return []\n",
    "        \n",
    "    time_series = data[\"Time Series (Daily)\"]\n",
    "    rows = []\n",
    "    \n",
    "    # Extract data points\n",
    "    for date_str, values in time_series.items():\n",
    "        row = (\n",
    "            symbol,\n",
    "            datetime.strptime(date_str, \"%Y-%m-%d\").date(),\n",
    "            float(values[\"1. open\"]),\n",
    "            float(values[\"2. high\"]),\n",
    "            float(values[\"3. low\"]),\n",
    "            float(values[\"4. close\"]),\n",
    "            int(values[\"5. volume\"]),\n",
    "            datetime.now()\n",
    "        )\n",
    "        rows.append(row)\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Testing timestamp types\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Print the type of current_timestamp() Spark SQL function\n",
    "print(f\"current_timestamp() is type: {type(current_timestamp())}\")\n",
    "\n",
    "# Print the type of datetime.now() Python function\n",
    "current_time = datetime.now()\n",
    "print(f\"datetime.now() is type: {type(current_time)}\")\n",
    "\n",
    "# Create a small test DataFrame to see how both are handled\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Create a test schema with TimestampType\n",
    "test_schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Test with Python datetime\n",
    "python_rows = [(\"test1\", datetime.now())]\n",
    "python_df = spark.createDataFrame(python_rows, test_schema)\n",
    "print(\"DataFrame created with Python datetime:\")\n",
    "python_df.printSchema()\n",
    "python_df.show(truncate=False)\n",
    "\n",
    "# Test with Spark current_timestamp - This works differently\n",
    "# We need to create a DataFrame first and then add the timestamp column\n",
    "spark_df = spark.createDataFrame([(\"test2\",)], [\"id\"])\n",
    "spark_df = spark_df.withColumn(\"timestamp\", current_timestamp())\n",
    "print(\"DataFrame created with Spark current_timestamp():\")\n",
    "spark_df.printSchema()\n",
    "spark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bronze Layer Ingestion Function\n",
    "def ingest_to_bronze():\n",
    "    \"\"\"Ingest stock data to bronze layer\"\"\"\n",
    "    # Get API key from Azure Key Vault\n",
    "    api_key = get_secret(\"alpha-vantage-api-key\", \"ALPHA_VANTAGE_API_KEY\")\n",
    "    \n",
    "    # Create empty dataframe with schema\n",
    "    bronze_df = spark.createDataFrame([], stock_schema)\n",
    "    \n",
    "    # Process each symbol with delay to avoid API limits\n",
    "    for symbol in symbols:\n",
    "        print(f\"Fetching data for {symbol}...\")\n",
    "        rows = fetch_alpha_vantage_data(symbol, api_key)\n",
    "        \n",
    "        if rows:\n",
    "            # Create DataFrame for this symbol\n",
    "            symbol_df = spark.createDataFrame(rows, stock_schema)\n",
    "            \n",
    "            # Union with main DataFrame\n",
    "            bronze_df = bronze_df.union(symbol_df)\n",
    "            print(f\"Added {symbol_df.count()} rows for {symbol}\")\n",
    "        \n",
    "        # Delay to respect API limits\n",
    "        import time\n",
    "        time.sleep(12)  # Alpha Vantage free tier allows 5 requests per minute\n",
    "    \n",
    "    # Write to bronze layer\n",
    "    bronze_df.write.format(\"delta\").mode(\"append\").partitionBy(\"symbol\") \\\n",
    "        .save(bronze_path)\n",
    "    \n",
    "    print(f\"Bronze layer updated with {bronze_df.count()} records\")\n",
    "    return bronze_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
